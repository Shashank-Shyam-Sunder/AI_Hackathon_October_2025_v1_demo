# ğŸ§  AI Cost Estimator â€” CLI & Dockerized Pipeline

## ğŸš€ Overview

AI Cost Estimator is an interactive, terminal-based pipeline that collects app requirements, detects relevant tasks from a catalog (using LLMs), and generates minimum/maximum cost estimates.

**Workflow:**
```
intake (interactive) â†’ task_detect (LLM-based) â†’ plan_and_cost
```

Each run automatically creates a timestamped folder under:
```
data/out/<project_name>-YYYYMMDD_HHMMSS/
```

This folder stores all generated artifacts:
- `intake_*.json` â€” user input
- `selected_tasks_*.json` â€” detected tasks
- `explain_*.md` â€” reasoning summary
- `cost_breakdown_*.json` â€” per-task cost details
- `cost_summary_*.md` â€” formatted final report
- `run_meta.json` â€” metadata (provider, timestamp, etc.)

---
## ğŸ¥ Demo Video

You can watch the complete 30-minute demo video on Google Drive using the link below:  
ğŸ‘‰ **[View the full demo on Google Drive](https://drive.google.com/file/d/1R5a-CEUYsN_agAPOhq_4JV1Nux45H4XW/view?usp=sharing)**

> ğŸ’¡ Tip: For a smoother viewing experience, watch the video at **2Ã— speed**.
---
## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw                     # Input catalogs
â”‚   â”‚   â””â”€â”€ AI_Cost_Estimator_Catalog_UPDATED.xlsx
â”‚   â””â”€â”€ out                     # Generated outputs (auto-created)
â”œâ”€â”€ docs
â”‚   â””â”€â”€ ARCHITECTURE.md         # System design and workflow
â”œâ”€â”€ src
â”‚   â””â”€â”€ app
â”‚       â”œâ”€â”€ cli/                # CLI entrypoints
â”‚       â”œâ”€â”€ core/               # Core logic (intake, detect, cost)
â”‚       â”œâ”€â”€ llm/                # LLM provider configs and selectors
â”‚       â””â”€â”€ utils/              # Internal helpers
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ example_prompts.json    # Example prompts to test the app
â”‚   â”œâ”€â”€ generate_root_folder_tree.py
â”‚   â””â”€â”€ clean_folder_structure.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ§° Prerequisites

- **Docker** (includes Docker Compose on Windows/macOS)
- **Internet connection** (required for LLM task detection)

---

## âš™ï¸ Environment Setup

Copy the example environment file and fill your API keys:
```bash
cp .env.example .env
```

`.env.example` contains:
```dotenv
OPENAI_API_KEY=your-openai-api-key
GOOGLE_API_KEY=your-google-api-key
PERPLEXITY_API_KEY=your-perplexity-api-key
GROK_API_KEY=your-grok-api-key
```

> **Note:**  
> When using an LLM for task detection, slight variability in results is expected â€” even with fixed parameters (temperature=0, top_p=1).  
> This is more noticeable with **Perplexity**, while **OpenAI** tends to be more stable.  
>  
> The **Google** and **Grok** integrations are experimental and not fully tested yet â€” you can try them and share feedback.

---

## ğŸª„ Quick Start â€” Run with Docker Compose

1ï¸âƒ£ **Build the image**
```bash
docker compose build
```

2ï¸âƒ£ **Run interactively**
```bash
docker compose run --rm app
```

Youâ€™ll be prompted for project details directly in your terminal.  
The pipeline will automatically run all stages and write outputs to:
```
data/out/<name>-<timestamp>/
```

| Location         | Path                |
| ---------------- | ------------------- |
| On host          | `./data/out/...`    |
| Inside container | `/app/data/out/...` |

---

## ğŸ’¡ Example Prompts

You can explore **sample prompts** for various application domains in:
```
utils/example_prompts.json
```

Use any of them to test the MVP and see how the **task detection** adapts across different use cases.

---

## ğŸ§© Optional â€” Run Individual Steps

| Step        | Command                                                                                                 |
| ------------ | ------------------------------------------------------------------------------------------------------- |
| Intake       | `docker compose run --rm app python -m src.app.core.cli_intake`                                         |
| Detect Tasks | `docker compose run --rm app python -m src.app.core.task_detect --run-dir "/app/data/out/<your-run>"`   |
| Plan & Cost  | `docker compose run --rm app python -m src.app.core.plan_and_cost --run-dir "/app/data/out/<your-run>"` |

---

## ğŸ’» Local Development (without Docker)

If you prefer running locally:

```bash
python -m venv .venv
.\.venv\Scripts\activate       # Windows
source .venv/bin/activate        # macOS/Linux

pip install --upgrade pip
pip install -r requirements

python -m src.app.cli.pipeline
```

Outputs will still be saved to `data/out/`.

---
## ğŸ“˜ Documentation

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for high-level design and pipeline flow.

For a detailed overview of current issues, known limitations, and possible improvements,  
please see [docs/CURRENT_ISSUES_AND_POSSIBLE_SOLUTIONS.md](docs/CURRENT_ISSUES_AND_POSSIBLE_SOLUTIONS.md).

---

## ğŸªª License

See the [LICENSE](LICENSE) file for license details.

---

## ğŸ—ï¸ Project Info

This CLI-based prototype was developed for the **Hyperskill Hackathon â€” October 2025**.  
Future updates will include:
- Automated LLM-based report generation
- Expanded task catalog
- Web dashboard for visualization
